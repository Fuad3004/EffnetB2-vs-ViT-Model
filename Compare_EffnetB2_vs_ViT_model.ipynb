{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Briging Pyhton_file from my git\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "https://github.com/Fuad3004/ComupterVision-food101-Dataset.git"
      ],
      "metadata": {
        "id": "PO2_1zl-Ngdm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOKSY337K_S9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e15a51a-5e0b-45ff-be2b-78746c2a5b68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n",
            "[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\n",
            "Cloning into 'ComupterVision-food101-Dataset'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 27 (delta 3), reused 14 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (27/27), 8.13 MiB | 5.58 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from python_file import data_setup, traintest\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/Fuad3004/ComupterVision-food101-Dataset.git\n",
        "    !mv ComupterVision-food101-Dataset/python_file .\n",
        "\n",
        "    !mv ComupterVision-food101-Dataset/helper_functions.py . # get the helper_functions.py script\n",
        "\n",
        "    !rm -rf ComupterVision-food101-Dataset\n",
        "    \n",
        "    from python_file import data_setup, traintest\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls python_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaVfko0KNJj4",
        "outputId": "36e9fc96-0b0d-4a55-efca-d05f096a1fbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_setup.py  __pycache__  tinyvggmodel.py\n",
            "evaluate.py    saving.py    traintest.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5uSqfno3M1H1",
        "outputId": "f1c44b60-0592-4b08-c156-a66aadce09dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset Downloading \n",
        "git link: https://github.com/Fuad3004/5-Classes-of-Food-101-Dataset/raw/main/5_Classes_of_Food101_Dataset.zip"
      ],
      "metadata": {
        "id": "U-BUohJGNnZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def download_data(source: str,\n",
        "                  destination: str,\n",
        "                  remove_source: bool=True) -> Path:\n",
        "\n",
        "  data_path= Path(\"data/\")\n",
        "  image_path=data_path/destination\n",
        "\n",
        "  if image_path.is_dir():\n",
        "    print(f\"{image_path} already exist\")\n",
        "\n",
        "  else:\n",
        "  \n",
        "    image_path.mkdir(parents=True, exist_ok=True)\n",
        "    target_file=Path(source).name\n",
        "\n",
        "    with open(data_path / target_file, \"wb\") as f:\n",
        "      request= requests.get(source)\n",
        "      print(\"Downloading..\")\n",
        "      f.write(request.content)\n",
        "\n",
        "    with zipfile.ZipFile(data_path/ target_file, \"r\") as zip_ref:\n",
        "      print(\"Unzipping\")\n",
        "      zip_ref.extractall(image_path)\n",
        "      print(\"Done\")\n",
        "\n",
        "  # if remove_source:\n",
        "  #   os.remove(data_path / target_file)\n",
        "\n",
        "  return image_path"
      ],
      "metadata": {
        "id": "H5HZSyWmM21c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path= download_data( source= \"https://github.com/Fuad3004/5-Classes-of-Food-101-Dataset/raw/main/5_Classes_of_Food101_Dataset.zip\",\n",
        "                          destination=\"5-classes-of-food101\"\n",
        "                          )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdDz8ZQuNzvJ",
        "outputId": "03111e8c-ba60-490b-bca0-d8d7b0e5ffaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading..\n",
            "Unzipping\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\""
      ],
      "metadata": {
        "id": "PxO5VDabN1Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FoodVision Mini model deployment experiment outline"
      ],
      "metadata": {
        "id": "CGMVKnRzO4Lf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Want a performs well & fast\n",
        "\n",
        "performance well : 95%= accruacy\n",
        "fast: as close to real-time(or faster) (30fps or 1/30 ms latency)\n",
        "\n",
        "---\n",
        "\n",
        "Compare the EffNetB2 model vs ViT model"
      ],
      "metadata": {
        "id": "Xl1zcmb3PoZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating an EffNetB2 feature extractor"
      ],
      "metadata": {
        "id": "L9D-xyN-ams4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extractor- A term for a transfer learning model that has its base layers frozen and output layers (head layers) customized to a certain problem."
      ],
      "metadata": {
        "id": "98NSfF1aayVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision \n",
        "torchvision.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LX7hpOs_N2xy",
        "outputId": "7994cf87-3555-42ac-887d-7188c331872e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.15.1+cu118'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Setup pretrained EffNetB2 weights\n",
        "effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "\n",
        "# 2. Get EffNetB2 transforms\n",
        "effnetb2_transforms = effnetb2_weights.transforms() #to transform our own data as like the original data that used on effnetb2 \n",
        "\n",
        "# 3. Setup pretrained model isntance\n",
        "effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights) # could also use weights=\"DEFAULT\"\n",
        "\n",
        "# 4. Freeze the base layers in the model (this will freeze all layers to begin with)\n",
        "for param in effnetb2.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULalGCS6bxd0",
        "outputId": "c1ec5c70-7fcd-4a27-a350-80533ace698a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b2_rwightman-bcdf34b7.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b2_rwightman-bcdf34b7.pth\n",
            "100%|██████████| 35.2M/35.2M [00:00<00:00, 86.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out EffNetB2 classifier head\n",
        "effnetb2.classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kvj9vNfpdTxz",
        "outputId": "50bf0a2c-c2f0-4850-a1b2-abaf23101c44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Dropout(p=0.3, inplace=True)\n",
              "  (1): Linear(in_features=1408, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from torchinfo import summary\n",
        "\n",
        "# summary(effnetb2, \n",
        "#         input_size=(1, 3, 224, 224),\n",
        "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "#         col_width=20,\n",
        "#         row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "KZaDUV1udcGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "thousan classes are there. Lets customise it"
      ],
      "metadata": {
        "id": "_SK6UBMLdvkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Update the classifier head\n",
        "effnetb2.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.3, inplace=True), # keep dropout layer same #probability=0.3\n",
        "    nn.Linear(in_features=1408, # keep in_features same \n",
        "              out_features=3)) # change out_features to suit our number of classes"
      ],
      "metadata": {
        "id": "Umdb9eLzdkGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2.classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcRBd-RfeHCR",
        "outputId": "d21e724c-bc6a-4db3-b0b6-55eb9de1e10f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Dropout(p=0.3, inplace=True)\n",
              "  (1): Linear(in_features=1408, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#effNetB2 function creating"
      ],
      "metadata": {
        "id": "pDhzGfYK7P6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_effnetb2_model(num_classes:int=5, \n",
        "                          seed:int=42):\n",
        "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int, optional): number of classes in the classifier head. \n",
        "            Defaults to 3.\n",
        "        seed (int, optional): random seed value. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        model (torch.nn.Module): EffNetB2 feature extractor model. \n",
        "        transforms (torchvision.transforms): EffNetB2 image transforms.\n",
        "    \"\"\"\n",
        "    # 1, 2, 3. Create EffNetB2 pretrained weights, transforms and model\n",
        "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "    transforms = weights.transforms()\n",
        "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "    # 4. Freeze all layers in base model\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # 5. Change classifier head with random seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.3, inplace=True),\n",
        "        nn.Linear(in_features=1408, out_features=num_classes), #I had to know about the value of in features! \n",
        "    )\n",
        "    \n",
        "    return model, transforms"
      ],
      "metadata": {
        "id": "PzGOswYOeJqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=5,\n",
        "                                                      seed=42) #model=effnetb2, effnetb2_transforms=transforms"
      ],
      "metadata": {
        "id": "02s_B_w_7hS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2.classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpK0ycFbHgpu",
        "outputId": "0ec76ba6-ae42-463c-a23d-fbac5ee84980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Dropout(p=0.3, inplace=True)\n",
              "  (1): Linear(in_features=1408, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2_transforms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eJ9EW-3Hqjk",
        "outputId": "54566a3a-5388-480d-c082-f403a9eb5c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ImageClassification(\n",
              "    crop_size=[288]\n",
              "    resize_size=[288]\n",
              "    mean=[0.485, 0.456, 0.406]\n",
              "    std=[0.229, 0.224, 0.225]\n",
              "    interpolation=InterpolationMode.BICUBIC\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#dataloader for EffNetB2\n",
        "get data ready for the model"
      ],
      "metadata": {
        "id": "DI3EJckuLx27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Setup DataLoaders\n",
        "from python_file import data_setup\n",
        "train_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                                                 test_dir=test_dir,\n",
        "                                                                                                 transform=effnetb2_transforms,\n",
        "                                                                                                 batch_size=32) "
      ],
      "metadata": {
        "id": "dAWyppV9HxwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataloader_effnetb2), len(test_dataloader_effnetb2),class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dexWliQjMqhj",
        "outputId": "323ff07e-9faf-4402-9799-9dfd980491a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24, 24, ['chicken_wings', 'cup_cakes', 'pizza', 'ramen', 'waffles'])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training EffNetB2 feature extractor\n",
        "\n",
        "#loss function\n",
        "\n",
        "#optimiser\n",
        "\n",
        "#Training Function"
      ],
      "metadata": {
        "id": "dLn8PIdmO0Pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from python_file import traintest\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = torch.optim.Adam(params=effnetb2.parameters(),\n",
        "                             lr=1e-3)\n",
        "# Setup loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Set seeds for reproducibility and train the model\n",
        "set_seeds()\n",
        "effnetb2_results = traintest.train(model=effnetb2,\n",
        "                                train_dataloader=train_dataloader_effnetb2,\n",
        "                                test_dataloader=test_dataloader_effnetb2,\n",
        "                                epochs=10,\n",
        "                                optimizer=optimizer,\n",
        "                                loss_fn=loss_fn,\n",
        "                                device=device)"
      ],
      "metadata": {
        "id": "LfnqzWVup6mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "plot_loss_curves(effnetb2_results)"
      ],
      "metadata": {
        "id": "DwxbRovBcyEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from python_file import saving\n",
        "\n",
        "# Save the model\n",
        "saving.save_model(model=effnetb2,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
      ],
      "metadata": {
        "id": "4ynxxX_Bc2q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get the model size in bytes then convert to megabytes\n",
        "pretrained_effnetb2_model_size = Path(\"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \n",
        "print(f\"Pretrained EffNetB2 feature extractor model size: {pretrained_effnetb2_model_size} MB\")"
      ],
      "metadata": {
        "id": "-VR2eW0uduvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary with EffNetB2 statistics\n",
        "effnetb2_stats = {\"test_loss\": effnetb2_results[\"test_loss\"][-1],\n",
        "                  \"test_acc\": effnetb2_results[\"test_acc\"][-1],\n",
        "                  \"number_of_parameters\": effnetb2_total_params,\n",
        "                  \"model_size (MB)\": pretrained_effnetb2_model_size}\n",
        "effnetb2_stats"
      ],
      "metadata": {
        "id": "78CUqHA_qFp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yci5sMQ1qFm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vit "
      ],
      "metadata": {
        "id": "kPWVnlKSyVO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "find the head layer"
      ],
      "metadata": {
        "id": "beMQV1U8yYTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out ViT heads layer\n",
        "vit = torchvision.models.vit_b_16()\n",
        "vit.heads"
      ],
      "metadata": {
        "id": "FtrxBp9wqFYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit"
      ],
      "metadata": {
        "id": "4t2DISkCyjYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vit_model(num_classes:int=3, \n",
        "                     seed:int=42):\n",
        "    \"\"\"Creates a ViT-B/16 feature extractor model and transforms.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int, optional): number of target classes. Defaults to 3.\n",
        "        seed (int, optional): random seed value for output layer. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        model (torch.nn.Module): ViT-B/16 feature extractor model. \n",
        "        transforms (torchvision.transforms): ViT-B/16 image transforms.\n",
        "    \"\"\"\n",
        "    # Create ViT_B_16 pretrained weights, transforms and model\n",
        "    weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
        "    transforms = weights.transforms()\n",
        "    model = torchvision.models.vit_b_16(weights=weights)\n",
        "\n",
        "    # Freeze all layers in model\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Change classifier head to suit our needs (this will be trainable)\n",
        "    torch.manual_seed(seed)\n",
        "    model.heads = nn.Sequential(nn.Linear(in_features=768, # keep this the same as original model\n",
        "                                          out_features=num_classes)) # update to reflect target number of classes\n",
        "    \n",
        "    return model, transforms"
      ],
      "metadata": {
        "id": "YTkY8UHlyjVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ViT model and transforms\n",
        "vit, vit_transforms = create_vit_model(num_classes=3,\n",
        "                                       seed=42)"
      ],
      "metadata": {
        "id": "1jgajfowyjSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# # Print ViT feature extractor model summary (uncomment for full output)\n",
        "# summary(vit, \n",
        "#         input_size=(1, 3, 224, 224),\n",
        "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "#         col_width=20,\n",
        "#         row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "Es-6xRMByjP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup ViT DataLoaders\n",
        "from going_modular.going_modular import data_setup\n",
        "train_dataloader_vit, test_dataloader_vit, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                                       test_dir=test_dir,\n",
        "                                                                                       transform=vit_transforms,\n",
        "                                                                                       batch_size=32)"
      ],
      "metadata": {
        "id": "xUiGrFkqyjND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = torch.optim.Adam(params=vit.parameters(),\n",
        "                             lr=1e-3)\n",
        "# Setup loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train ViT model with seeds set for reproducibility\n",
        "set_seeds()\n",
        "vit_results = engine.train(model=vit,\n",
        "                           train_dataloader=train_dataloader_vit,\n",
        "                           test_dataloader=test_dataloader_vit,\n",
        "                           epochs=10,\n",
        "                           optimizer=optimizer,\n",
        "                           loss_fn=loss_fn,\n",
        "                           device=device)"
      ],
      "metadata": {
        "id": "BwfRQAIryjKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "plot_loss_curves(vit_results)"
      ],
      "metadata": {
        "id": "73y-ldZfhLuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "from going_modular.going_modular import utils\n",
        "\n",
        "utils.save_model(model=vit,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=\"09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
      ],
      "metadata": {
        "id": "_tnfnOBYhLqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get the model size in bytes then convert to megabytes\n",
        "pretrained_vit_model_size = Path(\"models/09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \n",
        "print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")"
      ],
      "metadata": {
        "id": "zQC_eWcthLli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of parameters in ViT\n",
        "vit_total_params = sum(torch.numel(param) for param in vit.parameters())\n",
        "vit_total_params"
      ],
      "metadata": {
        "id": "bwYHEEl7DNVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ViT statistics dictionary\n",
        "vit_stats = {\"test_loss\": vit_results[\"test_loss\"][-1],\n",
        "             \"test_acc\": vit_results[\"test_acc\"][-1],\n",
        "             \"number_of_parameters\": vit_total_params,\n",
        "             \"model_size (MB)\": pretrained_vit_model_size}\n",
        "\n",
        "vit_stats"
      ],
      "metadata": {
        "id": "sY-hIXHXDPcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Making predictions with our trained models and timing them"
      ],
      "metadata": {
        "id": "I784HOcfrrR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get all test data paths\n",
        "print(f\"[INFO] Finding all filepaths ending with '.jpg' in directory: {test_dir}\")\n",
        "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
        "test_data_paths[:5]"
      ],
      "metadata": {
        "id": "SZtLg54Mrqoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Creating a function to make predictions across the test dataset"
      ],
      "metadata": {
        "id": "ZBHGmcFDsGVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import torch\n",
        "\n",
        "from PIL import Image\n",
        "from timeit import default_timer as timer \n",
        "from tqdm.auto import tqdm\n",
        "from typing import List, Dict\n",
        " \n",
        "# 1. Create a function to return a list of dictionaries with sample, truth label, prediction, prediction probability and prediction time\n",
        "def pred_and_store(paths: List[pathlib.Path], \n",
        "                   model: torch.nn.Module,\n",
        "                   transform: torchvision.transforms, \n",
        "                   class_names: List[str], \n",
        "                   device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\") -> List[Dict]:\n",
        "    \n",
        "    # 2. Create an empty list to store prediction dictionaires\n",
        "    pred_list = []\n",
        "    \n",
        "    # 3. Loop through target paths\n",
        "    for path in tqdm(paths):\n",
        "        \n",
        "        # 4. Create empty dictionary to store prediction information for each sample\n",
        "        pred_dict = {}\n",
        "\n",
        "        # 5. Get the sample path and ground truth class name\n",
        "        pred_dict[\"image_path\"] = path\n",
        "        class_name = path.parent.stem #.prerent is the file path .stem takes the final bits\n",
        "        pred_dict[\"class_name\"] = class_name\n",
        "        \n",
        "        # 6. Start the prediction timer\n",
        "        start_time = timer()\n",
        "        \n",
        "        # 7. Open image path\n",
        "        img = Image.open(path)\n",
        "        \n",
        "        # 8. Transform the image, add batch dimension and put image on target device\n",
        "        transformed_image = transform(img).unsqueeze(0).to(device) \n",
        "        \n",
        "        # 9. Prepare model for inference by sending it to target device and turning on eval() mode\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        \n",
        "        # 10. Get prediction probability, predicition label and prediction class\n",
        "        with torch.inference_mode():\n",
        "            pred_logit = model(transformed_image) # perform inference on target sample \n",
        "            pred_prob = torch.softmax(pred_logit, dim=1) # turn logits into prediction probabilities\n",
        "            pred_label = torch.argmax(pred_prob, dim=1) # turn prediction probabilities into prediction label # trnsor\n",
        "            pred_class = class_names[pred_label.cpu()] # hardcode prediction class to be on CPU #python variables live on the gpu \n",
        "\n",
        "            # 11. Make sure things in the dictionary are on CPU (required for inspecting predictions later on) \n",
        "            pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4) #pred_prob er max value ta nicchi\n",
        "            pred_dict[\"pred_class\"] = pred_class\n",
        "            \n",
        "            # 12. End the timer and calculate time per pred\n",
        "            end_time = timer()\n",
        "               [\"time_for_pred\"] = round(end_time-start_time, 4)\n",
        "\n",
        "        # 13. Does the pred match the true label?\n",
        "        pred_dict[\"correct\"] = class_name == pred_class\n",
        "\n",
        "        # 14. Add the dictionary to the list of preds\n",
        "        pred_list.append(pred_dict)\n",
        "    \n",
        "    # 15. Return list of prediction dictionaries\n",
        "    return pred_list"
      ],
      "metadata": {
        "id": "lxXe8Fjus0XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Making and timing predictions with ViT"
      ],
      "metadata": {
        "id": "xy81iVd3OXB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions across test dataset with EffNetB2\n",
        "effnetb2_test_pred_dicts = pred_and_store(paths=test_data_paths,\n",
        "                                          model=effnetb2,\n",
        "                                          transform=effnetb2_transforms,\n",
        "                                          class_names=class_names,\n",
        "                                          device=\"cpu\") # make predictions on CPU "
      ],
      "metadata": {
        "id": "uhmrnVrpsb8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the first 2 prediction dictionaries\n",
        "effnetb2_test_pred_dicts[:2]"
      ],
      "metadata": {
        "id": "s8nfS3azvhsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn the test_pred_dicts into a DataFrame\n",
        "import pandas as pd\n",
        "effnetb2_test_pred_df = pd.DataFrame(effnetb2_test_pred_dicts)\n",
        "effnetb2_test_pred_df.head()"
      ],
      "metadata": {
        "id": "B_xTG4aFvhp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check number of correct predictions\n",
        "effnetb2_test_pred_df.correct.value_counts()"
      ],
      "metadata": {
        "id": "UuHymg9-M1DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the average time per prediction \n",
        "effnetb2_average_time_per_pred = round(effnetb2_test_pred_df.time_for_pred.mean(), 4) #4 significant unit\n",
        "print(f\"EffNetB2 average time per prediction: {effnetb2_average_time_per_pred} seconds\")"
      ],
      "metadata": {
        "id": "BKEXm1msM-U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add EffNetB2 average prediction time to stats dictionary \n",
        "effnetb2_stats[\"time_per_pred_cpu\"] = effnetb2_average_time_per_pred\n",
        "effnetb2_stats"
      ],
      "metadata": {
        "id": "STjqwC2vNTsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Making and timing predictions with ViT"
      ],
      "metadata": {
        "id": "pGHoHJP1N12D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make list of prediction dictionaries with ViT feature extractor model on test images\n",
        "vit_test_pred_dicts = pred_and_store(paths=test_data_paths,\n",
        "                                     model=vit,\n",
        "                                     transform=vit_transforms,\n",
        "                                     class_names=class_names,\n",
        "                                     device=\"cpu\") #haedcode device to cpu because not sure if gpu available when we deploy "
      ],
      "metadata": {
        "id": "FptkjXscN2_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the first couple of ViT predictions on the test dataset\n",
        "vit_test_pred_dicts[:2]"
      ],
      "metadata": {
        "id": "HSqgmVJHN9a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn vit_test_pred_dicts into a DataFrame\n",
        "import pandas as pd\n",
        "vit_test_pred_df = pd.DataFrame(vit_test_pred_dicts)\n",
        "vit_test_pred_df.head()"
      ],
      "metadata": {
        "id": "c2RD8KFYOQ2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of correct predictions\n",
        "vit_test_pred_df.correct.value_counts()"
      ],
      "metadata": {
        "id": "bV4V7sUSOUtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average time per prediction for ViT model\n",
        "vit_average_time_per_pred = round(vit_test_pred_df.time_for_pred.mean(), 4)\n",
        "print(f\"ViT average time per prediction: {vit_average_time_per_pred} seconds\")"
      ],
      "metadata": {
        "id": "W37mguuzOkI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add average prediction time for ViT model on CPU\n",
        "vit_stats[\"time_per_pred_cpu\"] = vit_average_time_per_pred\n",
        "vit_stats"
      ],
      "metadata": {
        "id": "gzaWb-jwOmKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Comparing model results, prediction times and size"
      ],
      "metadata": {
        "id": "B7cxmbp7OmHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn stat dictionaries into DataFrame\n",
        "df = pd.DataFrame([effnetb2_stats, vit_stats])\n",
        "\n",
        "# Add column for model names\n",
        "df[\"model\"] = [\"EffNetB2\", \"ViT\"]\n",
        "\n",
        "# Convert accuracy to percentages\n",
        "df[\"test_acc\"] = round(df[\"test_acc\"] * 100, 2)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "IOTOdefyOmDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare ViT to EffNetB2 across different characteristics\n",
        "pd.DataFrame(data=(df.set_index(\"model\").loc[\"ViT\"] / df.set_index(\"model\").loc[\"EffNetB2\"]), # divide ViT statistics by EffNetB2 statistics\n",
        "             columns=[\"ViT to EffNetB2 ratios\"]).T #T to transpose it"
      ],
      "metadata": {
        "id": "h8KYFcx7RZBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualizing the speed vs. performance tradeoff"
      ],
      "metadata": {
        "id": "jBzJBUGemC8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a plot from model comparison DataFrame\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "scatter = ax.scatter(data=df, \n",
        "                     x=\"time_per_pred_cpu\", \n",
        "                     y=\"test_acc\", \n",
        "                     c=[\"blue\", \"orange\"], # what colours to use?\n",
        "                     s=\"model_size (MB)\") # size the dots by the model sizes\n",
        "\n",
        "# 2. Add titles, labels and customize fontsize for aesthetics\n",
        "ax.set_title(\"FoodVision Mini Inference Speed vs Performance\", fontsize=18)\n",
        "ax.set_xlabel(\"Prediction time per image (seconds)\", fontsize=14)\n",
        "ax.set_ylabel(\"Test accuracy (%)\", fontsize=14)\n",
        "ax.tick_params(axis='both', labelsize=12)\n",
        "ax.grid(True)\n",
        "\n",
        "# 3. Annotate with model names\n",
        "for index, row in df.iterrows():\n",
        "    ax.annotate(text=row[\"model\"], # note: depending on your version of Matplotlib, you may need to use \"s=...\" or \"text=...\", see: https://github.com/faustomorales/keras-ocr/issues/183#issuecomment-977733270 \n",
        "                xy=(row[\"time_per_pred_cpu\"]+0.0006, row[\"test_acc\"]+0.03),\n",
        "                size=12) \n",
        "\n",
        "# 4. Create a legend based on model sizes\n",
        "handles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.5)\n",
        "model_size_legend = ax.legend(handles, \n",
        "                              labels, \n",
        "                              loc=\"lower right\", \n",
        "                              title=\"Model size (MB)\",\n",
        "                              fontsize=12)\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig(\"09-foodvision-mini-inference-speed-vs-performance.jpg\") #or try this images/09-foodvision-mini-inference-speed-vs-performance.jpg\n",
        "\n",
        "# Show the figure\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Vu-qlAX7l_WS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}